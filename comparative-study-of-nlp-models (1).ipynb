{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027713,
     "end_time": "2021-05-25T14:21:36.203700",
     "exception": false,
     "start_time": "2021-05-25T14:21:36.175987",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TEXT CLASSIFICATION METHODS COMPARISON\n",
    "Hi,\n",
    "This is my 2nd notebook on NLP, and I want to address the COVID-19 text classification problem. What I mainly wanted to do, is to create a comparative study of the different methods used for NLP text classification/semantic analysis. You can copy my notebook, make changes here and there, and let me know how it goes. I'll also include comments for each step of what I'm doing. Also, I'm available for suggestions/corrections, so do comment if you have any. \n",
    "\n",
    "*This is going to be a long one, let's go!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026312,
     "end_time": "2021-05-25T14:21:36.257636",
     "exception": false,
     "start_time": "2021-05-25T14:21:36.231324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import Libraries\n",
    "Pretty straigtforward, we'll start with importing the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.029308,
     "end_time": "2021-05-25T14:21:36.315203",
     "exception": false,
     "start_time": "2021-05-25T14:21:36.285895",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading the dataset\n",
    "We'll use pandas to read the train and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:36.384543Z",
     "iopub.status.busy": "2021-05-25T14:21:36.383770Z",
     "iopub.status.idle": "2021-05-25T14:21:46.625021Z",
     "shell.execute_reply": "2021-05-25T14:21:46.623559Z",
     "shell.execute_reply.started": "2021-05-25T12:31:53.210384Z"
    },
    "papermill": {
     "duration": 10.281892,
     "end_time": "2021-05-25T14:21:46.625313",
     "exception": false,
     "start_time": "2021-05-25T14:21:36.343421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU,SimpleRNN\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D, Input\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:46.690514Z",
     "iopub.status.busy": "2021-05-25T14:21:46.689803Z",
     "iopub.status.idle": "2021-05-25T14:21:47.077355Z",
     "shell.execute_reply": "2021-05-25T14:21:47.076803Z",
     "shell.execute_reply.started": "2021-05-25T12:32:40.149017Z"
    },
    "papermill": {
     "duration": 0.424585,
     "end_time": "2021-05-25T14:21:47.077511",
     "exception": false,
     "start_time": "2021-05-25T14:21:46.652926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv', encoding = 'latin1') \n",
    "test = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv', encoding = 'latin1')\n",
    "\n",
    "#Now you can try without the encoding (which I had done before), it throws an error, something like this:  'utf-8' codec can't decode byte <byte> in position <position>: unexpected end of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:47.147680Z",
     "iopub.status.busy": "2021-05-25T14:21:47.147005Z",
     "iopub.status.idle": "2021-05-25T14:21:47.168877Z",
     "shell.execute_reply": "2021-05-25T14:21:47.169497Z"
    },
    "papermill": {
     "duration": 0.065106,
     "end_time": "2021-05-25T14:21:47.169672",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.104566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UserName</th>\n",
       "      <th>ScreenName</th>\n",
       "      <th>Location</th>\n",
       "      <th>TweetAt</th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3799</td>\n",
       "      <td>48751</td>\n",
       "      <td>London</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3800</td>\n",
       "      <td>48752</td>\n",
       "      <td>UK</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3801</td>\n",
       "      <td>48753</td>\n",
       "      <td>Vagabonds</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3802</td>\n",
       "      <td>48754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3803</td>\n",
       "      <td>48755</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-03-2020</td>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Extremely Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   UserName  ScreenName   Location     TweetAt  \\\n",
       "0      3799       48751     London  16-03-2020   \n",
       "1      3800       48752         UK  16-03-2020   \n",
       "2      3801       48753  Vagabonds  16-03-2020   \n",
       "3      3802       48754        NaN  16-03-2020   \n",
       "4      3803       48755        NaN  16-03-2020   \n",
       "\n",
       "                                       OriginalTweet           Sentiment  \n",
       "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
       "1  advice Talk to your neighbours family to excha...            Positive  \n",
       "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
       "3  My food stock is not the only one which is emp...            Positive  \n",
       "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:47.228305Z",
     "iopub.status.busy": "2021-05-25T14:21:47.227338Z",
     "iopub.status.idle": "2021-05-25T14:21:47.233983Z",
     "shell.execute_reply": "2021-05-25T14:21:47.234438Z"
    },
    "papermill": {
     "duration": 0.037446,
     "end_time": "2021-05-25T14:21:47.234616",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.197170",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02731,
     "end_time": "2021-05-25T14:21:47.289610",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.262300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, having read our train and test data, let's get the max number of words in a sentence. We'd need this for padding (explained in later section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:47.355547Z",
     "iopub.status.busy": "2021-05-25T14:21:47.350278Z",
     "iopub.status.idle": "2021-05-25T14:21:47.471997Z",
     "shell.execute_reply": "2021-05-25T14:21:47.471360Z"
    },
    "papermill": {
     "duration": 0.154925,
     "end_time": "2021-05-25T14:21:47.472137",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.317212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['OriginalTweet'].apply(lambda x:len(str(x).split())).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028271,
     "end_time": "2021-05-25T14:21:47.528555",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.500284",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's check out the unique output classes of the data. We'll store them in a variable to use it for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:47.594354Z",
     "iopub.status.busy": "2021-05-25T14:21:47.593346Z",
     "iopub.status.idle": "2021-05-25T14:21:47.599911Z",
     "shell.execute_reply": "2021-05-25T14:21:47.599258Z",
     "shell.execute_reply.started": "2021-05-25T12:32:49.96858Z"
    },
    "papermill": {
     "duration": 0.043052,
     "end_time": "2021-05-25T14:21:47.600057",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.557005",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutral', 'Positive', 'Extremely Negative', 'Negative',\n",
       "       'Extremely Positive'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = train['Sentiment'].unique()\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028095,
     "end_time": "2021-05-25T14:21:47.656516",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.628421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert categorical variable into dummy/indicator variables.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "**pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False, dtype=None) **\n",
    "\n",
    "Parameters:\n",
    "dataarray-like, Series, or DataFrame\n",
    "Data of which to get dummy indicators.\n",
    "\n",
    "prefixstr, list of str, or dict of str, default None\n",
    "String to append DataFrame column names. Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternatively, prefix can be a dictionary mapping column names to prefixes.\n",
    "\n",
    "prefix_sepstr, default ‘_’\n",
    "If appending prefix, separator/delimiter to use. Or pass a list or dictionary as with prefix.\n",
    "\n",
    "dummy_nabool, default False\n",
    "Add a column to indicate NaNs, if False NaNs are ignored.\n",
    "\n",
    "columnslist-like, default None\n",
    "Column names in the DataFrame to be encoded. If columns is None then all the columns with object or category dtype will be converted.\n",
    "\n",
    "sparsebool, default False\n",
    "Whether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).\n",
    "\n",
    "drop_firstbool, default False\n",
    "Whether to get k-1 dummies out of k categorical levels by removing the first level.\n",
    "\n",
    "dtypedtype, default np.uint8\n",
    "Data type for new columns. Only a single dtype is allowed.\n",
    "\n",
    "Returns\n",
    "DataFrame\n",
    "Dummy-coded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:47.722726Z",
     "iopub.status.busy": "2021-05-25T14:21:47.722048Z",
     "iopub.status.idle": "2021-05-25T14:21:47.735375Z",
     "shell.execute_reply": "2021-05-25T14:21:47.734666Z",
     "shell.execute_reply.started": "2021-05-25T12:33:09.123691Z"
    },
    "papermill": {
     "duration": 0.050477,
     "end_time": "2021-05-25T14:21:47.735523",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.685046",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor:        Extremely Negative  Extremely Positive  Negative  Neutral  Positive\n",
      "0                       0                   0         0        1         0\n",
      "1                       0                   0         0        0         1\n",
      "2                       0                   0         0        0         1\n",
      "3                       0                   0         0        0         1\n",
      "4                       1                   0         0        0         0\n",
      "...                   ...                 ...       ...      ...       ...\n",
      "41152                   0                   0         0        1         0\n",
      "41153                   1                   0         0        0         0\n",
      "41154                   0                   0         0        0         1\n",
      "41155                   0                   0         0        1         0\n",
      "41156                   0                   0         1        0         0\n",
      "\n",
      "[41157 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "y=train['Sentiment'].values\n",
    "y = pd.get_dummies(y)\n",
    "print('Shape of label tensor:', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028599,
     "end_time": "2021-05-25T14:21:47.794257",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.765658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Tokenization\n",
    "Tokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types – word, character, and subword (n-gram characters) tokenization.\n",
    "\n",
    "Here, we use keras.processing class Tokenizer.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', char_level=False, oov_token=None,\n",
    "    document_count=0, **kwargs\n",
    ")\n",
    "\n",
    "**num_words**: max number of words to be kept based on word frequency. \n",
    "\n",
    "**filters**: removing special characters from the data\n",
    "\n",
    "**lower**: convert to lowecase\n",
    "\n",
    "**split**: split the data on ' '\n",
    "\n",
    "**char_level**: (Boolean value true/false) whether every character has to be treated as a token.\n",
    "\n",
    "**document_count**: An integer count of the total number of documents that were used to fit the Tokenizer\n",
    "\n",
    "\n",
    "So, next we would use this tokenizer to convert the text into sequences and to ensure a unifrom length, we pad these sequences to the max_len with 0s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028574,
     "end_time": "2021-05-25T14:21:47.851658",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.823084",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoding Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02846,
     "end_time": "2021-05-25T14:21:47.912010",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.883550",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We would tokenize our entire data, so I'd create a new dataframe combining the tweet values of both train and test data, and fit our tokenizer on this new dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028477,
     "end_time": "2021-05-25T14:21:47.969939",
     "exception": false,
     "start_time": "2021-05-25T14:21:47.941462",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, having initalized a tokenizer in the previous step, we would now use the tokenizer to convert the text from train dataset to tokens, and pad the values with 0s to ensure a uniform length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:48.039984Z",
     "iopub.status.busy": "2021-05-25T14:21:48.039206Z",
     "iopub.status.idle": "2021-05-25T14:21:48.732772Z",
     "shell.execute_reply": "2021-05-25T14:21:48.732200Z",
     "shell.execute_reply.started": "2021-05-25T12:35:16.449105Z"
    },
    "papermill": {
     "duration": 0.730675,
     "end_time": "2021-05-25T14:21:48.732920",
     "exception": false,
     "start_time": "2021-05-25T14:21:48.002245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24677"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = train['OriginalTweet'] + test['OriginalTweet']\n",
    "tmp = tmp.astype(str)\n",
    "tokenizer = text.Tokenizer(num_words=400000,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True,\n",
    "    split=\" \")\n",
    "max_len = 70\n",
    "tokenizer.fit_on_texts(tmp)\n",
    "word_index = tokenizer.word_index\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:48.794954Z",
     "iopub.status.busy": "2021-05-25T14:21:48.794298Z",
     "iopub.status.idle": "2021-05-25T14:21:50.646844Z",
     "shell.execute_reply": "2021-05-25T14:21:50.646261Z",
     "shell.execute_reply.started": "2021-05-25T12:35:20.228793Z"
    },
    "papermill": {
     "duration": 1.884315,
     "end_time": "2021-05-25T14:21:50.646993",
     "exception": false,
     "start_time": "2021-05-25T14:21:48.762678",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (41157, 70)\n"
     ]
    }
   ],
   "source": [
    "X = train['OriginalTweet'].values\n",
    "X = tokenizer.texts_to_sequences(X)\n",
    "X = sequence.pad_sequences(X, maxlen=max_len)\n",
    "print('Shape of data tensor:', X.shape)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028814,
     "end_time": "2021-05-25T14:21:50.705178",
     "exception": false,
     "start_time": "2021-05-25T14:21:50.676364",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Splitting the data\n",
    "We would split up our train dataset into xtrain, xvalid, ytrain and yvalid. Let's go parameter by parameter.\n",
    "\n",
    "Syntax:\n",
    "**sklearn.model_selection.train_test_split(*arrays, test_size=None, train_size=None, random_state=None, shuffle=True, stratify=None)\n",
    "test_size\n",
    "train_size\n",
    "shuffle**\n",
    "\n",
    "***arrays**:\n",
    "\n",
    "train.OriginalTweet.values: X value (Input data on which the model has to be trained), train.Sentiment.values: y value (Output data on which the model has to be trained),\n",
    "\n",
    "\n",
    "**stratify**: If not None, data is split in a stratified fashion, using this as the class labels.\n",
    "\n",
    "**test_size**: Dividing the train and test data set (In our case 20%)\n",
    "\n",
    "**train_size**: Dividing the train and test data set (In our case 100% - 20% = 80%)\n",
    "\n",
    "**random_state**: value for initializing the internal random number generator, which will decide the splitting of data into train and test indices in your case\n",
    "\n",
    "\n",
    "**shuffle**: how the train and test data is divided. (In this case: 20%)\n",
    "\n",
    "**stratify**:If not None, data is split in a stratified fashion, using this as the class labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:50.773051Z",
     "iopub.status.busy": "2021-05-25T14:21:50.772344Z",
     "iopub.status.idle": "2021-05-25T14:21:50.787235Z",
     "shell.execute_reply": "2021-05-25T14:21:50.786534Z",
     "shell.execute_reply.started": "2021-05-25T13:14:52.609126Z"
    },
    "papermill": {
     "duration": 0.052226,
     "end_time": "2021-05-25T14:21:50.787417",
     "exception": false,
     "start_time": "2021-05-25T14:21:50.735191",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                  random_state=46, \n",
    "                                                  test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:50.853499Z",
     "iopub.status.busy": "2021-05-25T14:21:50.852696Z",
     "iopub.status.idle": "2021-05-25T14:21:50.857156Z",
     "shell.execute_reply": "2021-05-25T14:21:50.856513Z"
    },
    "papermill": {
     "duration": 0.040416,
     "end_time": "2021-05-25T14:21:50.857328",
     "exception": false,
     "start_time": "2021-05-25T14:21:50.816912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (28809, 70)  y_train.shape: (28809, 5)\n",
      "x_test.shape: (12348, 70)  y_train.shape: (12348, 5)\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape: ' + str(x_train.shape),' y_train.shape: '+str(y_train.shape))\n",
    "print('x_test.shape: ' + str(x_test.shape),' y_train.shape: '+str(y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039705,
     "end_time": "2021-05-25T14:21:50.934002",
     "exception": false,
     "start_time": "2021-05-25T14:21:50.894297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Embedding Layer\n",
    "\n",
    "To move onto the next step, we need to be familiar with the concept of word embeddings.\n",
    "\n",
    "> Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network, and hence the technique is often lumped into the field of deep learning.\n",
    "\n",
    "In simple words, we can put it in this way that each word is represented as a vector in vector space, and that's how we maintain the similarity between the words. Consider two similar words, like 'good' and 'great'. The distance of the vectors between these words need to be less to denote their similarity. There are a number of techniques to convert words to vectors, such as:\n",
    "\n",
    "1. Frequency based Embedding\n",
    "\n",
    " 1.1. Count Vectors\n",
    " \n",
    " 1.2. TF-IDF\n",
    " \n",
    " 1.3. Co-Occurrence Matrix\n",
    " \n",
    "2. Prediction based Embedding\n",
    "\n",
    " 2.1. CBOW\n",
    " \n",
    " 2.2. Skip-Gram\n",
    "\n",
    "3. Using pre-trained Word Vectors\n",
    "\n",
    "  3.1. Word2Vec\n",
    "  \n",
    "  3.2. GloVe\n",
    "  \n",
    "I won't go into much detail regarding the embeddings, but if you want to know more, you should definitely check out this [link](https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/)\n",
    "\n",
    "Also, while choosing an embedding, there's no right or wrong. It completely depends on the problem statement. I'll be trying out the default [pretrained Keras Embedding Layer](https://keras.io/api/layers/core_layers/embedding/) and the pretrained [GloVe vectors](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010) here. GloVe works excellent when the data size is huge, as they compare the words to a giant global corpus. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033208,
     "end_time": "2021-05-25T14:21:51.002326",
     "exception": false,
     "start_time": "2021-05-25T14:21:50.969118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I've included the GloVe vector file in input data. Let me just initialize and build the embedding matrix, which would serve as weights in Embedding layer of my neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:21:51.072228Z",
     "iopub.status.busy": "2021-05-25T14:21:51.071468Z",
     "iopub.status.idle": "2021-05-25T14:27:06.201343Z",
     "shell.execute_reply": "2021-05-25T14:27:06.200683Z",
     "shell.execute_reply.started": "2021-05-25T12:46:50.12766Z"
    },
    "papermill": {
     "duration": 315.166469,
     "end_time": "2021-05-25T14:27:06.201542",
     "exception": false,
     "start_time": "2021-05-25T14:21:51.035073",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2196018it [05:15, 6969.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2196017 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('../input/glove840b300dtxt/glove.840B.300d.txt','r',encoding='utf-8')\n",
    "for line in tqdm(f):\n",
    "    values = line.split(' ')\n",
    "    word = values[0]\n",
    "    coefs = np.asarray([float(val) for val in values[1:]])\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:27:08.090361Z",
     "iopub.status.busy": "2021-05-25T14:27:08.089515Z",
     "iopub.status.idle": "2021-05-25T14:27:08.300667Z",
     "shell.execute_reply": "2021-05-25T14:27:08.301540Z",
     "shell.execute_reply.started": "2021-05-25T12:51:04.624683Z"
    },
    "papermill": {
     "duration": 1.153677,
     "end_time": "2021-05-25T14:27:08.301760",
     "exception": false,
     "start_time": "2021-05-25T14:27:07.148083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24677/24677 [00:00<00:00, 119637.65it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.965052,
     "end_time": "2021-05-25T14:27:10.206227",
     "exception": false,
     "start_time": "2021-05-25T14:27:09.241175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Building\n",
    "\n",
    "This is the part where it gets interesting. After all the process of cleaning, tokenizing, embedding words, we're now ready to create our model and feed the data into it and check which model gives a better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.937043,
     "end_time": "2021-05-25T14:27:12.074034",
     "exception": false,
     "start_time": "2021-05-25T14:27:11.136991",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**1. Simple RNN (Recurrent Neural Networks) Model**\n",
    "\n",
    "We'll start off with a very simple RNN model. If you're new to the concept of tensorflow, have a [quick look](https://www.tensorflow.org/tutorials/quickstart/beginner)\n",
    "\n",
    "Let's first address the question. \n",
    "\n",
    "What is RNN?\n",
    "> > > Recurrent neural networks (RNN) are a class of neural networks that are helpful in modeling sequence data. Derived from feedforward networks, RNNs exhibit similar behavior to how human brains function. Simply put: recurrent neural networks produce predictive results in sequential data that other algorithms can’t.\n",
    "\n",
    "This is a very good [article](https://builtin.com/data-science/recurrent-neural-networks-and-lstm) to jumpstart with the concepts of RNN and LSTM and understand why they're in much popular demand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.927085,
     "end_time": "2021-05-25T14:27:13.976565",
     "exception": false,
     "start_time": "2021-05-25T14:27:13.049480",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Activation Function**\n",
    "> In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be \"ON\" (1) or \"OFF\" (0), depending on input. This is similar to the behavior of the linear perceptron in neural networks.[Continue reading..](https://missinglink.ai/guides/neural-network-concepts/7-types-neural-network-activation-functions-right/#:~:text=Activation%20functions%20are%20mathematical%20equations,relevant%20for%20the%20model's%20prediction.)\n",
    "\n",
    "**Optimizer**\n",
    "> They tie together the loss function and model parameters by updating the model in response to the output of the loss function. In simpler terms, optimizers shape and mold your model into its most accurate possible form by futzing with the weights. The loss function is the guide to the terrain, telling the optimizer when it’s moving in the right or wrong direction.\n",
    "[Continue Reading..](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6#:~:text=Many%20people%20may%20be%20using,help%20to%20get%20results%20faster)\n",
    "The [learning rate scheduler](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1) controls the learning rate of the model per epoch according to a predefined scheduler.\n",
    "\n",
    "**The Sequential Model**\n",
    "\n",
    "Let me just give a walkthrough on what a sequential model is.\n",
    "In simple words, sequential model is a linear stack of layers, where each layer represent some kind of input, output or computation.\n",
    "\n",
    "We'd be using the [sequential model from Keras](https://keras.io/guides/sequential_model/).\n",
    "\n",
    "If you checked out the above link, you might have encountered the 'Dense' layer everywhere. So, what exactly is it?\n",
    "\n",
    "> The dense layer is a neural network layer that is connected deeply, which means each neuron in the dense layer receives input from all neurons of its previous layer. The dense layer is found to be the most commonly used layer in the models. In the background, the dense layer performs a matrix-vector multiplication.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "tf.keras.layers.Dense(\n",
    "    units,\n",
    "    activation=None,\n",
    "    use_bias=True,\n",
    "    kernel_initializer=\"glorot_uniform\",\n",
    "    bias_initializer=\"zeros\",\n",
    "    kernel_regularizer=None,\n",
    "    bias_regularizer=None,\n",
    "    activity_regularizer=None,\n",
    "    kernel_constraint=None,\n",
    "    bias_constraint=None,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "**units**: Positive integer, dimensionality of the output space.\n",
    "\n",
    "**activation**: Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "\n",
    "**use_bias**: Boolean, whether the layer uses a bias vector.\n",
    "\n",
    "**kernel_initializer**: Initializer for the kernel weights matrix.\n",
    "\n",
    "**bias_initializer**: Initializer for the bias vector.\n",
    "\n",
    "**kernel_regularizer**: Regularizer function applied to the kernel weights matrix.\n",
    "\n",
    "**bias_regularizer**: Regularizer function applied to the bias vector.\n",
    "\n",
    "**activity_regularizer**: Regularizer function applied to the output of the layer (its \"activation\").\n",
    "\n",
    "**kernel_constraint**: Constraint function applied to the kernel weights matrix.\n",
    "\n",
    "**bias_constraint**: Constraint function applied to the bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:27:15.837258Z",
     "iopub.status.busy": "2021-05-25T14:27:15.836213Z",
     "iopub.status.idle": "2021-05-25T14:27:16.225535Z",
     "shell.execute_reply": "2021-05-25T14:27:16.226031Z"
    },
    "papermill": {
     "duration": 1.324722,
     "end_time": "2021-05-25T14:27:16.226230",
     "exception": false,
     "start_time": "2021-05-25T14:27:14.901508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 70, 32)            789696    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 803,501\n",
      "Trainable params: 803,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SimpleRNNModel = Sequential()\n",
    "SimpleRNNModel.add(Input(shape=x_train.shape[1]))\n",
    "SimpleRNNModel.add(Embedding(len(tokenizer.word_index)+1,32))\n",
    "SimpleRNNModel.add(SimpleRNN(100))\n",
    "SimpleRNNModel.add(Dense(5, activation='softmax'))\n",
    "#SimpleRNNModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "SimpleRNNModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','categorical_accuracy','AUC','Precision','Recall'])    \n",
    "SimpleRNNModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.93574,
     "end_time": "2021-05-25T14:27:18.091074",
     "exception": false,
     "start_time": "2021-05-25T14:27:17.155334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Fitting the data\n",
    "We would now [fit the model](https://keras.io/api/models/model_training_apis/) on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:27:20.025632Z",
     "iopub.status.busy": "2021-05-25T14:27:20.024939Z",
     "iopub.status.idle": "2021-05-25T14:28:28.005321Z",
     "shell.execute_reply": "2021-05-25T14:28:28.004698Z"
    },
    "papermill": {
     "duration": 68.956759,
     "end_time": "2021-05-25T14:28:28.005480",
     "exception": false,
     "start_time": "2021-05-25T14:27:19.048721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "406/406 [==============================] - 16s 34ms/step - loss: 1.5388 - accuracy: 0.2842 - categorical_accuracy: 0.2842 - auc: 0.6250 - precision: 0.4271 - recall: 0.0077 - val_loss: 1.4841 - val_accuracy: 0.3211 - val_categorical_accuracy: 0.3211 - val_auc: 0.6753 - val_precision: 0.4603 - val_recall: 0.0403\n",
      "Epoch 2/5\n",
      "406/406 [==============================] - 13s 31ms/step - loss: 1.2374 - accuracy: 0.4871 - categorical_accuracy: 0.4871 - auc: 0.7997 - precision: 0.7021 - recall: 0.1643 - val_loss: 1.4203 - val_accuracy: 0.4193 - val_categorical_accuracy: 0.4193 - val_auc: 0.7780 - val_precision: 0.4528 - val_recall: 0.3398\n",
      "Epoch 3/5\n",
      "406/406 [==============================] - 13s 32ms/step - loss: 0.7167 - accuracy: 0.7508 - categorical_accuracy: 0.7508 - auc: 0.9392 - precision: 0.8242 - recall: 0.6474 - val_loss: 1.2349 - val_accuracy: 0.4932 - val_categorical_accuracy: 0.4932 - val_auc: 0.8174 - val_precision: 0.5391 - val_recall: 0.4068\n",
      "Epoch 4/5\n",
      "406/406 [==============================] - 13s 31ms/step - loss: 0.3314 - accuracy: 0.9014 - categorical_accuracy: 0.9014 - auc: 0.9874 - precision: 0.9239 - recall: 0.8748 - val_loss: 1.5075 - val_accuracy: 0.4790 - val_categorical_accuracy: 0.4790 - val_auc: 0.7988 - val_precision: 0.5002 - val_recall: 0.4353\n",
      "Epoch 5/5\n",
      "406/406 [==============================] - 13s 32ms/step - loss: 0.1493 - accuracy: 0.9608 - categorical_accuracy: 0.9608 - auc: 0.9973 - precision: 0.9667 - recall: 0.9532 - val_loss: 1.7965 - val_accuracy: 0.4852 - val_categorical_accuracy: 0.4852 - val_auc: 0.7907 - val_precision: 0.5008 - val_recall: 0.4585\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SimpleRNNModelResults = SimpleRNNModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.235109,
     "end_time": "2021-05-25T14:28:30.485760",
     "exception": false,
     "start_time": "2021-05-25T14:28:29.250651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Overfitting Model Alert!\n",
    "So, I see the accuracy is quite high ~95%, and also high precision, recall and AUC but the val_accuracy is not that great. This means that there might be a possibility of overfitting, where in the model performs well with the train data, but while performing with new data which it isn't trained with, it might not be performing quite well. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.239311,
     "end_time": "2021-05-25T14:28:32.964747",
     "exception": false,
     "start_time": "2021-05-25T14:28:31.725436",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Preprocessing the test dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.239145,
     "end_time": "2021-05-25T14:28:35.442378",
     "exception": false,
     "start_time": "2021-05-25T14:28:34.203233",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**2. LSTM (Long Short Term Memory) Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.260273,
     "end_time": "2021-05-25T14:28:37.982223",
     "exception": false,
     "start_time": "2021-05-25T14:28:36.721950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> Long short-term memory is an artificial recurrent neural network architecture used in the field of deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. It can not only process single data points, but also entire sequences of data.\n",
    "\n",
    "Technically, LSTM was built to overcome the [vanishing gradient](https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577) issue encountered in RNNs. In simple words, RNN training is something like this. From each layer, the error is backpropagated to update the weights of previous layers, but in a case where the gradient is exponentially so less, that it becomes insignificant and the weights are not updated at all. We call this as the *vanishing gradient* problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.229477,
     "end_time": "2021-05-25T14:28:40.456311",
     "exception": false,
     "start_time": "2021-05-25T14:28:39.226834",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I'd first try with a simple LSTM model, with a very similar architecture as of the simple RNN model and check how much accuracy that gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:28:42.942078Z",
     "iopub.status.busy": "2021-05-25T14:28:42.941369Z",
     "iopub.status.idle": "2021-05-25T14:28:43.232057Z",
     "shell.execute_reply": "2021-05-25T14:28:43.231257Z"
    },
    "papermill": {
     "duration": 1.538102,
     "end_time": "2021-05-25T14:28:43.232259",
     "exception": false,
     "start_time": "2021-05-25T14:28:41.694157",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 70, 32)            789696    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 843,401\n",
      "Trainable params: 843,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SimpleLSTMModel = Sequential()\n",
    "SimpleLSTMModel.add(Input(shape=x_train.shape[1]))\n",
    "SimpleLSTMModel.add(Embedding(len(tokenizer.word_index)+1,32))\n",
    "SimpleLSTMModel.add(LSTM(100))\n",
    "SimpleLSTMModel.add(Dense(5, activation='softmax'))\n",
    "SimpleLSTMModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "SimpleLSTMModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:28:45.770425Z",
     "iopub.status.busy": "2021-05-25T14:28:45.763230Z",
     "iopub.status.idle": "2021-05-25T14:32:01.363558Z",
     "shell.execute_reply": "2021-05-25T14:32:01.362918Z"
    },
    "papermill": {
     "duration": 196.846095,
     "end_time": "2021-05-25T14:32:01.363749",
     "exception": false,
     "start_time": "2021-05-25T14:28:44.517654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "406/406 [==============================] - 41s 97ms/step - loss: 1.4744 - accuracy: 0.3371 - val_loss: 0.9917 - val_accuracy: 0.6123\n",
      "Epoch 2/5\n",
      "406/406 [==============================] - 39s 95ms/step - loss: 0.8280 - accuracy: 0.6847 - val_loss: 0.8184 - val_accuracy: 0.6886\n",
      "Epoch 3/5\n",
      "406/406 [==============================] - 39s 96ms/step - loss: 0.5866 - accuracy: 0.7955 - val_loss: 0.8220 - val_accuracy: 0.7098\n",
      "Epoch 4/5\n",
      "406/406 [==============================] - 39s 95ms/step - loss: 0.4643 - accuracy: 0.8481 - val_loss: 0.7973 - val_accuracy: 0.7195\n",
      "Epoch 5/5\n",
      "406/406 [==============================] - 38s 94ms/step - loss: 0.3902 - accuracy: 0.8790 - val_loss: 0.8157 - val_accuracy: 0.7188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SimpleLSTMModelResults = SimpleLSTMModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.89803,
     "end_time": "2021-05-25T14:32:05.097678",
     "exception": false,
     "start_time": "2021-05-25T14:32:03.199648",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So you see, the improvement is noticable changing the model from Simple RNN to LSTM. This is majorly because of the learning structure of LSTM. Let's try with a GRU model and check how much accuracy that provides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.85281,
     "end_time": "2021-05-25T14:32:08.805807",
     "exception": false,
     "start_time": "2021-05-25T14:32:06.952997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**3. GRU Gated Recurrent Units**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 1.896118,
     "end_time": "2021-05-25T14:32:12.545326",
     "exception": false,
     "start_time": "2021-05-25T14:32:10.649208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Gated recurrent units are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. GRU consists of a update and forget gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:32:16.236889Z",
     "iopub.status.busy": "2021-05-25T14:32:16.235846Z",
     "iopub.status.idle": "2021-05-25T14:32:16.496792Z",
     "shell.execute_reply": "2021-05-25T14:32:16.496192Z"
    },
    "papermill": {
     "duration": 2.114171,
     "end_time": "2021-05-25T14:32:16.496944",
     "exception": false,
     "start_time": "2021-05-25T14:32:14.382773",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 70, 32)            789696    \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 100)               40200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 505       \n",
      "=================================================================\n",
      "Total params: 830,401\n",
      "Trainable params: 830,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "SimpleGRUModel = Sequential()\n",
    "SimpleGRUModel.add(Input(shape=x_train.shape[1]))\n",
    "SimpleGRUModel.add(Embedding(len(tokenizer.word_index)+1,32))\n",
    "SimpleGRUModel.add(GRU(100))\n",
    "SimpleGRUModel.add(Dense(5, activation='softmax'))\n",
    "SimpleGRUModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "SimpleGRUModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:32:20.255821Z",
     "iopub.status.busy": "2021-05-25T14:32:20.255026Z",
     "iopub.status.idle": "2021-05-25T14:35:27.293630Z",
     "shell.execute_reply": "2021-05-25T14:35:27.294388Z"
    },
    "papermill": {
     "duration": 188.929833,
     "end_time": "2021-05-25T14:35:27.294677",
     "exception": false,
     "start_time": "2021-05-25T14:32:18.364844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "406/406 [==============================] - 38s 88ms/step - loss: 1.5255 - accuracy: 0.3011 - val_loss: 1.0215 - val_accuracy: 0.5724\n",
      "Epoch 2/5\n",
      "406/406 [==============================] - 37s 91ms/step - loss: 0.8309 - accuracy: 0.6814 - val_loss: 0.8109 - val_accuracy: 0.6935\n",
      "Epoch 3/5\n",
      "406/406 [==============================] - 36s 89ms/step - loss: 0.5436 - accuracy: 0.8149 - val_loss: 0.7731 - val_accuracy: 0.7161\n",
      "Epoch 4/5\n",
      "406/406 [==============================] - 37s 90ms/step - loss: 0.4285 - accuracy: 0.8637 - val_loss: 0.8367 - val_accuracy: 0.7178\n",
      "Epoch 5/5\n",
      "406/406 [==============================] - 39s 97ms/step - loss: 0.3374 - accuracy: 0.8925 - val_loss: 0.8504 - val_accuracy: 0.7258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SimpleGRUModelResults = SimpleGRUModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 2.556011,
     "end_time": "2021-05-25T14:35:32.482355",
     "exception": false,
     "start_time": "2021-05-25T14:35:29.926344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**4. Bidirectional LSTM Model**\n",
    "\n",
    "The Bidirectional LSTM or BiLSTM is a modified version of a simple LSTM, where we use 2 LSTM models, one processing the input and learning occuring in a forward direction and one for backward. It is proven better in terms of accuracy than traditional RNN/GRU and LSTM.\n",
    "\n",
    "If you're confused about model selection for your dataset, you can refer [this discussion thread](https://datascience.stackexchange.com/questions/25650/what-is-lstm-bilstm-and-when-to-use-them#:~:text=BiLSTM%20means%20bidirectional%20LSTM%2C%20which,this%20architecture%20to%20other%20RNNs.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:35:37.540012Z",
     "iopub.status.busy": "2021-05-25T14:35:37.535299Z",
     "iopub.status.idle": "2021-05-25T14:35:38.157120Z",
     "shell.execute_reply": "2021-05-25T14:35:38.157651Z"
    },
    "papermill": {
     "duration": 3.173084,
     "end_time": "2021-05-25T14:35:38.157852",
     "exception": false,
     "start_time": "2021-05-25T14:35:34.984768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 70, 32)            789696    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 70, 200)           106400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 1005      \n",
      "=================================================================\n",
      "Total params: 897,101\n",
      "Trainable params: 897,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BILSTMModel = Sequential()\n",
    "BILSTMModel.add(Input(shape=x_train.shape[1]))\n",
    "BILSTMModel.add(Embedding(len(tokenizer.word_index)+1,32))\n",
    "BILSTMModel.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "BILSTMModel.add(GlobalMaxPooling1D())\n",
    "BILSTMModel.add(Dense(5, activation='softmax'))\n",
    "BILSTMModel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "BILSTMModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:35:43.074364Z",
     "iopub.status.busy": "2021-05-25T14:35:43.073640Z",
     "iopub.status.idle": "2021-05-25T14:40:21.373355Z",
     "shell.execute_reply": "2021-05-25T14:40:21.372600Z"
    },
    "papermill": {
     "duration": 280.75053,
     "end_time": "2021-05-25T14:40:21.373524",
     "exception": false,
     "start_time": "2021-05-25T14:35:40.622994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "406/406 [==============================] - 59s 135ms/step - loss: 1.4757 - accuracy: 0.3377 - val_loss: 0.9785 - val_accuracy: 0.6147\n",
      "Epoch 2/5\n",
      "406/406 [==============================] - 55s 136ms/step - loss: 0.8167 - accuracy: 0.6981 - val_loss: 0.7668 - val_accuracy: 0.7070\n",
      "Epoch 3/5\n",
      "406/406 [==============================] - 54s 133ms/step - loss: 0.5833 - accuracy: 0.7976 - val_loss: 0.7346 - val_accuracy: 0.7338\n",
      "Epoch 4/5\n",
      "406/406 [==============================] - 56s 138ms/step - loss: 0.4695 - accuracy: 0.8473 - val_loss: 0.7733 - val_accuracy: 0.7206\n",
      "Epoch 5/5\n",
      "406/406 [==============================] - 54s 133ms/step - loss: 0.3912 - accuracy: 0.8764 - val_loss: 0.7966 - val_accuracy: 0.7254\n"
     ]
    }
   ],
   "source": [
    "BILSTMModelResults = BILSTMModel.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.104538,
     "end_time": "2021-05-25T14:40:27.614917",
     "exception": false,
     "start_time": "2021-05-25T14:40:24.510379",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter Tuning/ Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 3.091683,
     "end_time": "2021-05-25T14:40:33.805021",
     "exception": false,
     "start_time": "2021-05-25T14:40:30.713338",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's try some tweaking parameters here and there and adding more layers to see if we can improve the accuracy. I'll also use the weights of the Embedding layer coming from GloVe vector which was initialized in an earlier step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:40:39.997074Z",
     "iopub.status.busy": "2021-05-25T14:40:39.996014Z",
     "iopub.status.idle": "2021-05-25T14:40:40.450612Z",
     "shell.execute_reply": "2021-05-25T14:40:40.451678Z"
    },
    "papermill": {
     "duration": 3.535259,
     "end_time": "2021-05-25T14:40:40.451962",
     "exception": false,
     "start_time": "2021-05-25T14:40:36.916703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 70, 300)           7403400   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 70, 100)           140400    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 7,549,105\n",
      "Trainable params: 7,549,105\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BILSTMModel_2 = Sequential()\n",
    "BILSTMModel_2.add(Input(shape=x_train.shape[1]))\n",
    "BILSTMModel_2.add(Embedding(24678,300, weights=[embedding_matrix]))\n",
    "BILSTMModel_2.add(Bidirectional(LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1)))\n",
    "BILSTMModel_2.add(GlobalMaxPooling1D())\n",
    "BILSTMModel_2.add(Dense(50, activation='relu'))\n",
    "BILSTMModel_2.add(Dropout(0.2))\n",
    "BILSTMModel_2.add(Dense(5, activation='softmax'))\n",
    "BILSTMModel_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "BILSTMModel_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-25T14:40:46.646704Z",
     "iopub.status.busy": "2021-05-25T14:40:46.645775Z",
     "iopub.status.idle": "2021-05-25T14:54:06.667238Z",
     "shell.execute_reply": "2021-05-25T14:54:06.666617Z"
    },
    "papermill": {
     "duration": 803.173225,
     "end_time": "2021-05-25T14:54:06.667437",
     "exception": false,
     "start_time": "2021-05-25T14:40:43.494212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "406/406 [==============================] - 166s 395ms/step - loss: 1.3436 - accuracy: 0.4037 - val_loss: 0.7827 - val_accuracy: 0.7150\n",
      "Epoch 2/5\n",
      "406/406 [==============================] - 158s 389ms/step - loss: 0.7012 - accuracy: 0.7503 - val_loss: 0.6556 - val_accuracy: 0.7563\n",
      "Epoch 3/5\n",
      "406/406 [==============================] - 159s 393ms/step - loss: 0.5255 - accuracy: 0.8281 - val_loss: 0.6400 - val_accuracy: 0.7723\n",
      "Epoch 4/5\n",
      "406/406 [==============================] - 158s 388ms/step - loss: 0.4183 - accuracy: 0.8662 - val_loss: 0.6732 - val_accuracy: 0.7549\n",
      "Epoch 5/5\n",
      "406/406 [==============================] - 159s 390ms/step - loss: 0.3294 - accuracy: 0.8985 - val_loss: 0.6930 - val_accuracy: 0.7667\n"
     ]
    }
   ],
   "source": [
    "BILSTMModel_2Results = BILSTMModel_2.fit(x_train, y_train, epochs=5, batch_size=64,validation_split=0.1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1966.951265,
   "end_time": "2021-05-25T14:54:17.045067",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-25T14:21:30.093802",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
